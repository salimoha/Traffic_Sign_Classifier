{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here we need to load the data, split into training and validation sets.\n",
    "# Then we have to preprocess the data (normalization, grayscale etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import flatten\n",
    "xglorot = False\n",
    "if xglorot:\n",
    "    def weight_variable(shape, mean, std, name):\n",
    "        fan_in = np.prod(shape[:-1])\n",
    "        initial = np.random.randn(*shape) / np.sqrt(fan_in / 2.)\n",
    "        return tf.Variable(initial, dtype = tf.float32)\n",
    "else:\n",
    "    def weight_variable(shape, mean, sigma, name):\n",
    "         return tf.Variable(tf.random_normal(shape, mean, sigma), name = name)\n",
    "\n",
    "def bias_variable(shape, name):\n",
    "    # create bias variable given 1-D shape of fan_out\n",
    "    return tf.Variable(tf.constant(0.1, shape = shape), dtype = tf.float32, name = name)\n",
    "def dropout(layer, keep_prob):\n",
    "    return tf.nn.dropout(layer, keep_prob)   \n",
    "\n",
    "# L1 through Lout define the depth of each of the layers.\n",
    "# Named variables and placeholders so the names can be used when the saved session is restored. \n",
    "# Named certain operations as well for the same reason\n",
    "#rate = 5*1e-4\n",
    "mu = 0\n",
    "sigma = 0.1\n",
    "# Here we set the layer depths\n",
    "L1 = 6\n",
    "L2 = 16\n",
    "L3 = 120\n",
    "L4 = 84\n",
    "Lout = 43\n",
    "image_depth = X_train.shape[3]\n",
    "KEEP_PROB1 = 1.0\n",
    "KEEP_PROB2 = 1.0\n",
    "\n",
    "x = tf.placeholder(tf.float32, (None, 32, 32, image_depth), name = \"x\")\n",
    "y = tf.placeholder(tf.int32, (None), name = \"y\")\n",
    "# Rate is defined as another placeholder, which will be fed for feed_dict\n",
    "rate = tf.placeholder(tf.float32, (1,), name = \"rate\")\n",
    "\n",
    "# SOLUTION: Layer 1: Convolutional. Input = 32x32x3. Output = 28x28xL1.\n",
    "conv1_W = weight_variable((5, 5, image_depth, L1), mu, sigma, \"conv1_W\")\n",
    "conv1_b = bias_variable([L1], \"conv1_b\")\n",
    "conv1   = tf.add(tf.nn.conv2d(x, conv1_W, strides=[1, 1, 1, 1], padding='VALID'), conv1_b, name = \"conv1_op\")\n",
    "#print(conv1.shape)\n",
    "#conv1_layer = tf.get_variable(\"conv1_layer\", dtype=tf.float32, \n",
    "#  initializer=conv1, validate_shape = False)\n",
    "\n",
    "# SOLUTION: Activation.\n",
    "conv1 = tf.nn.relu(conv1, name = \"conv1_relu\")\n",
    "\n",
    "# SOLUTION: Pooling. Input = 28x28xL1. Output = 14x14xL1.\n",
    "conv1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "# SOLUTION: Layer 2: Convolutional. Output = 10x10x16.\n",
    "#conv2_W = tf.Variable(tf.random_uniform(shape=(5, 5, 6, 16), minval=-2.45/np.sqrt(5*5*6*16), maxval=+2.45/np.sqrt(5*5*6*16)))\n",
    "conv2_W = weight_variable((5, 5, L1, L2), mu, sigma, \"conv2_W\")\n",
    "conv2_b = bias_variable([L2], \"conv2_b\")\n",
    "conv2   = tf.add(tf.nn.conv2d(conv1, conv2_W, strides=[1, 1, 1, 1], padding='VALID'), conv2_b, name = \"conv2_op\")\n",
    "#print(conv2.shape)\n",
    "#conv2_layer = tf.get_variable(\"conv2_layer\", dtype=tf.float32, \n",
    "# initializer=conv2, validate_shape = False)\n",
    "# SOLUTION: Activation.\n",
    "conv2 = tf.nn.relu(conv2, name = \"conv2_relu\")\n",
    "\n",
    "# SOLUTION: Pooling. Input = 10x10xL2. Output = 5x5xL2.\n",
    "conv2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "# SOLUTION: Flatten. Input = 5x5xL2. Output = 5*5*L2.\n",
    "fc0   = flatten(conv2)\n",
    "    \n",
    "# SOLUTION: Layer 3: Fully Connected. Input = 5*5*L2. Output = L3.\n",
    "#fc1_W = tf.Variable(tf.random_uniform(shape=(400, 120), minval=-2.45/np.sqrt(400*120), maxval=2.45/np.sqrt(400*120)))\n",
    "fc1_W = weight_variable((5*5*L2, L3), mu, sigma, \"fc1_W\")\n",
    "fc1_b = bias_variable([L3], \"fc1_b\")\n",
    "fc1   = tf.matmul(fc0, fc1_W) + fc1_b\n",
    "    \n",
    "# SOLUTION: Activation.\n",
    "fc1    = tf.nn.relu(fc1)\n",
    "fc1 = dropout(fc1, KEEP_PROB1)\n",
    "\n",
    "# SOLUTION: Layer 4: Fully Connected. Input = L3. Output = L4.\n",
    "#fc2_W  = tf.Variable(tf.random_uniform(shape=(120, 84), minval=-2.45/np.sqrt(400*120), maxval = 2.45/np.sqrt(400*120)))\n",
    "fc2_W = weight_variable((L3, L4), mu, sigma, \"fc2_W\")\n",
    "fc2_b  = bias_variable([L4], \"fc2_b\")\n",
    "fc2    = tf.matmul(fc1, fc2_W) + fc2_b\n",
    "    \n",
    "# SOLUTION: Activation.\n",
    "fc2    = tf.nn.relu(fc2)\n",
    "fc2 = dropout(fc2, KEEP_PROB2)\n",
    "\n",
    "# SOLUTION: Layer 5: Fully Connected. Input = L4. Output = Lout.\n",
    "fc3_W = weight_variable((L4, Lout), mu, sigma, \"fc3_W\")\n",
    "fc3_b  = bias_variable([Lout], \"fc3_b\")\n",
    "logits = tf.add(tf.matmul(fc2, fc3_W), fc3_b, name = \"logits_op\")\n",
    "\n",
    "\n",
    "one_hot_y = tf.one_hot(y, Lout, name = \"one_hot_op\")\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_y, logits=logits, name = \"cross_entropy_op\")\n",
    "loss_operation = tf.reduce_mean(cross_entropy, name = \"loss_op\")\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = rate)\n",
    "training_operation = optimizer.minimize(loss_operation)\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1), name = \"correct_pred_op\")\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name = \"accuracy_op\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Train your model here.\n",
    "### Calculate and report the accuracy on the training and validation set.\n",
    "### Once a final model architecture is selected, \n",
    "### the accuracy on the test set should be calculated and reported as well.\n",
    "### Feel free to use as many code cells as needed.\n",
    "#saver = tf.train.Saver()\n",
    "\n",
    "def evaluate(X_data, y_data, BATCH_SIZE, rate):\n",
    "    num_examples = len(X_data)\n",
    "    total_accuracy = 0\n",
    "    sess = tf.get_default_session()\n",
    "    for offset in range(0, num_examples, BATCH_SIZE):\n",
    "        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n",
    "        accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y, rate: rate})\n",
    "        total_accuracy += (accuracy * len(batch_x))\n",
    "    return total_accuracy / num_examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def error_function(X_train, y_train, X_valid, y_valid, rate, epochs, BATCH_SIZE):\n",
    "    with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    num_examples = len(X_train)\n",
    "    num_batches = np.int(num_examples / BATCH_SIZE)\n",
    "    print(\"Training...\")\n",
    "    for i in range(epochs):\n",
    "        avg_train_cost = 0\n",
    "        avg_valid_cost = 0\n",
    "        #X_train, y_train = shuffle(X_train, y_train)\n",
    "        for offset in range(0, num_examples, BATCH_SIZE):\n",
    "            end = offset + BATCH_SIZE\n",
    "            batch_x, batch_y = X_train[offset:end], y_train[offset:end]\n",
    "            sess.run([training_operation], feed_dict={x: batch_x, y: batch_y, rate: rate})\n",
    "            train_cost = sess.run(loss_operation, feed_dict={x:batch_x, y:batch_y})            \n",
    "            avg_train_cost += train_cost/ (num_batches)\n",
    "            \n",
    "        valid_cost = sess.run(loss_operation, feed_dict={x:X_valid, y:y_valid})    \n",
    "        epochs.append(i+1)\n",
    "        tc.append(avg_train_cost)\n",
    "        vc.append(valid_cost)\n",
    "        validation_accuracy = evaluate(X_valid, y_valid, BATCH_SIZE, rate)\n",
    "        training_accuracy = evaluate(X_train[:(num_examples)], y_train[:(num_examples)], BATCH_SIZE, rate)\n",
    "        ta.append(training_accuracy)\n",
    "        va.append(validation_accuracy)\n",
    "        print(\"EPOCH {} ...\".format(i+1))        \n",
    "        print(\"Average Training Cost = \", avg_train_cost)\n",
    "        print(\"Validation Cost = \", valid_cost)\n",
    "        print(\"Validation Accuracy = {:.3f}\".format(validation_accuracy))\n",
    "        print(\"Training Accuracy = {:.3f}\".format(training_accuracy))\n",
    "        print()\n",
    "    plt.figure(1), plt.title(\"Error vs Epochs\")\n",
    "    fig1 = plt.plot(epochs, tc, 'r-', label = \"Training Cost\")\n",
    "    plt.plot(epochs, vc, 'b-', label = \"Validation Cost\")\n",
    "    plt.legend(loc = \"upper right\"), plt.show()\n",
    "        \n",
    "    plt.figure(2), plt.title(\"Accuracy vs Epochs, Vacc : {:0.3f}\".format(validation_accuracy))\n",
    "    fig2 = plt.plot(epochs, ta, 'r-', label = \"Training Accuracy\")\n",
    "    plt.plot(epochs, va, 'b-', label = \"Validation Accuracy\")\n",
    "    plt.legend(loc = \"lower right\"), plt.show()\n",
    "    return valid_cost"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
